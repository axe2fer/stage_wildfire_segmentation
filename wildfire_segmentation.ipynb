{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iy5cSbQCNz7K",
    "outputId": "de218269-5a63-466a-d6a0-4dae1bf66837"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!pip install opencv-python tensorflow numpy pandas scikit-learn matplotlib rasterio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qDNyavpMICS"
   },
   "outputs": [],
   "source": [
    "# Import des librairies nécessaires\n",
    "import os\n",
    "import gdown\n",
    "from glob import glob\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import rasterio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e75e8f61"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "BASE_URL = 'https://drive.google.com/uc?id={}'\n",
    "BASE_PATH = os.path.abspath(\"C:/content\")  \n",
    "COMPRESSED_DIR = os.path.join(BASE_PATH, \"compressed\")\n",
    "\n",
    "REGIONS = {\n",
    "    'South_America': '1hnBUmYkvFYohTf9hTnKiyp2MXVevglK7',\n",
    "    'Oceania': '1xHYTICHKU0u3-kIrq-pM9k0YaeQt60Bt',\n",
    "    'North_America1': '1BXRGldTdGGNeWDOFqNnmiNPuPQjweB2M',\n",
    "    'North_America2': '1zW_pEIggJ5Li7uQX9XKMfHkcgL3kiUoi',\n",
    "    'Africa': '1Ng3JwsjJPApshk8lJdGcsNHI52NaEMDX',\n",
    "    'Europe': '1vANGtfuEdn0ZnILA6BYXW1_7jt8CU0gA',\n",
    "    'Asia1': '1xgOQkeQIswq3hLBhNzuNPTtsL4ZavuC3',\n",
    "    'Asia2': '1w_wv0_QZhnH9jO1ygJg6ssTrXupIbTHp',\n",
    "    'Asia3': '1heefSuPsnLZkNSJ2jTa4M-jGWo_9nAri',\n",
    "    'Asia4': '1lyR6y6u8tSozfv3AQJ1PuUcdR0BU9yUk',\n",
    "    'Asia5': '1Y1UysFrZ8AiugKvDpWI3nHcjo7-CQ4Dp',\n",
    "}\n",
    "\n",
    "SUBSET_SAMPLES = {\n",
    "    'samples': '1gwQdhXrxCybcO16vem09DfW5fPadAA_p',\n",
    "}\n",
    "\n",
    "def download_file(file_id, output):\n",
    "    \"\"\"Télécharge un fichier Google Drive par ID.\"\"\"\n",
    "    url = BASE_URL.format(file_id)\n",
    "    print(f\"Téléchargement depuis Google Drive : {output}\")\n",
    "    gdown.download(url, output, quiet=False, fuzzy=True)\n",
    "\n",
    "def download_regions(region_list, output_dir=COMPRESSED_DIR, subset=False):\n",
    "    \"\"\"\n",
    "    Télécharge les datasets spécifiés.\n",
    "\n",
    "    Args:\n",
    "        region_list (list): Liste des régions à télécharger (ex: [\"Asia2\", \"Asia3\"])\n",
    "        output_dir (str): Dossier de sortie\n",
    "        subset (bool): Si True, télécharge uniquement le subset samples.zip\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if subset:\n",
    "        # Téléchargement du subset\n",
    "        zip_file_name = 'samples.zip'\n",
    "        output = os.path.join(BASE_PATH, zip_file_name)\n",
    "        print(\"Téléchargement du subset d'échantillons...\")\n",
    "        download_file(SUBSET_SAMPLES['samples'], output)\n",
    "        return output\n",
    "\n",
    "    else:\n",
    "        downloaded_files = []\n",
    "        for region in region_list:\n",
    "            if region not in REGIONS:\n",
    "                print(f\"Région {region} inconnue. Ignorée.\")\n",
    "                continue\n",
    "\n",
    "            zip_file_name = f'{region}.zip'\n",
    "            output = os.path.join(output_dir, zip_file_name)\n",
    "            print(f\"=== Téléchargement de la région : {region} ===\")\n",
    "            download_file(REGIONS[region], output)\n",
    "            downloaded_files.append(output)\n",
    "        return downloaded_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x7gSG4VPRg-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Chemin de base pour Windows/Linux (adapte si besoin)\n",
    "BASE_PATH = os.path.abspath(\"C:/content\")\n",
    "COMPRESSED_DIR = os.path.join(BASE_PATH, \"compressed\")\n",
    "IMAGES_DIR = os.path.join(BASE_PATH, \"images\")\n",
    "MASKS_DIR = os.path.join(BASE_PATH, \"masks\")\n",
    "\n",
    "def unzip_dataset(full_dataset=True,\n",
    "                  samples_zip_path=os.path.join(BASE_PATH, \"samples.zip\"),\n",
    "                  compressed_dir=COMPRESSED_DIR,\n",
    "                  output_dir=BASE_PATH):\n",
    "    \"\"\"\n",
    "    Décompresse soit le full dataset (compressed/*.zip), soit le subset (samples.zip).\n",
    "\n",
    "    Args:\n",
    "        full_dataset (bool): Si True -> décompresse le dataset complet, sinon subset.\n",
    "        samples_zip_path (str): chemin vers le fichier samples.zip\n",
    "        compressed_dir (str): chemin vers les zips continents téléchargés\n",
    "        output_dir (str): dossier de sortie final\n",
    "    \"\"\"\n",
    "    if full_dataset:\n",
    "        print('Unzipping Full Dataset...')\n",
    "\n",
    "        # Création des répertoires fixes\n",
    "        patches_output_dir = os.path.join(IMAGES_DIR, 'patches')\n",
    "        masks_output_dir = os.path.join(MASKS_DIR, 'patches')\n",
    "        voting_output_dir = os.path.join(MASKS_DIR, 'voting')\n",
    "        intersection_output_dir = os.path.join(MASKS_DIR, 'intersection')\n",
    "\n",
    "        for d in [patches_output_dir, masks_output_dir, voting_output_dir, intersection_output_dir]:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "\n",
    "        zips_continents = glob(os.path.join(compressed_dir, '*.zip'))\n",
    "\n",
    "        tmp_dir = os.path.join(output_dir, 'tmp')\n",
    "        tmp_derivates = os.path.join(output_dir, 'tmp_derivates')\n",
    "\n",
    "        print(f'Unzip images to {patches_output_dir}')\n",
    "        print(f'Unzip masks to {masks_output_dir}')\n",
    "\n",
    "        total_files = 0\n",
    "        for zip_continent in zips_continents:\n",
    "            print(f'Unzipping: {zip_continent}')\n",
    "\n",
    "            os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "            with zipfile.ZipFile(zip_continent, 'r') as zip_ref:\n",
    "                print(f'Num zipped files: {len(zip_ref.namelist())}')\n",
    "                zip_ref.extractall(tmp_dir)\n",
    "\n",
    "            patches_zips = glob(os.path.join(tmp_dir, '*.zip'))\n",
    "            print(f'Num. of zips unpacked: {len(patches_zips)}')\n",
    "\n",
    "            print('Unzipping patches...')\n",
    "            num_files = 0\n",
    "            for patches_zip in patches_zips:\n",
    "                output_dir_zip = patches_output_dir\n",
    "\n",
    "                if patches_zip.endswith('masks_derivates.zip'):\n",
    "                    with zipfile.ZipFile(patches_zip, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(tmp_derivates)\n",
    "                        num_files += len(zip_ref.namelist())\n",
    "\n",
    "                    derivate_patches = glob(os.path.join(tmp_derivates, '*.tif'))\n",
    "\n",
    "                    for derivate_patch in derivate_patches:\n",
    "                        if '_voting_' in derivate_patch.lower():\n",
    "                            shutil.move(derivate_patch, derivate_patch.replace(tmp_derivates, voting_output_dir))\n",
    "                        elif '_intersection_' in derivate_patch.lower():\n",
    "                            shutil.move(derivate_patch, derivate_patch.replace(tmp_derivates, intersection_output_dir))\n",
    "\n",
    "                    shutil.rmtree(tmp_derivates)\n",
    "                    continue\n",
    "\n",
    "                if patches_zip.endswith('masks.zip'):\n",
    "                    output_dir_zip = masks_output_dir\n",
    "\n",
    "                with zipfile.ZipFile(patches_zip, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(output_dir_zip)\n",
    "                    num_files += len(zip_ref.namelist())\n",
    "\n",
    "            total_files += num_files\n",
    "            print(f'Zip: {zip_continent} - Patches: {num_files}')\n",
    "            shutil.rmtree(tmp_dir)\n",
    "\n",
    "        print(f'Total files unzipped: {total_files}')\n",
    "        print('Full Dataset Ready!')\n",
    "\n",
    "    else:\n",
    "        print(\"Unzipping subset samples...\")\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            with zipfile.ZipFile(samples_zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(tmpdirname)\n",
    "\n",
    "            images_path = IMAGES_DIR\n",
    "            masks_path = MASKS_DIR\n",
    "            manual_annotations_path = os.path.join(BASE_PATH, 'manual_annotations', 'patches')\n",
    "\n",
    "            for d in [images_path, masks_path, manual_annotations_path]:\n",
    "                os.makedirs(d, exist_ok=True)\n",
    "\n",
    "            image_zip = os.path.join(tmpdirname, 'samples', 'images', 'patches.zip')\n",
    "            with zipfile.ZipFile(image_zip, 'r') as zip_ref:\n",
    "                zip_ref.extractall(images_path)\n",
    "\n",
    "            masks_zips = glob(os.path.join(tmpdirname, 'samples', 'masks', '*.zip'))\n",
    "            for mask_zip in masks_zips:\n",
    "                with zipfile.ZipFile(mask_zip, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(masks_path)\n",
    "\n",
    "            masks = glob(os.path.join(masks_path, 'patches', '*_GOLI_v2_*.tif'))\n",
    "            for mask in masks:\n",
    "                mask_name = os.path.basename(mask)\n",
    "                os.rename(\n",
    "                    os.path.join(masks_path, 'patches', mask_name),\n",
    "                    os.path.join(masks_path, 'patches', mask_name.replace('GOLI_v2', 'Kumar-Roy'))\n",
    "                )\n",
    "\n",
    "            manual_annotations_zips = glob(os.path.join(tmpdirname, 'samples', 'manual_annotations', '*.zip'))\n",
    "            for manual_ann_zip in manual_annotations_zips:\n",
    "                with zipfile.ZipFile(manual_ann_zip, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(manual_annotations_path)\n",
    "\n",
    "        print(\"Subset unzipped successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KllxJdToI0f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retourne 2 listes :\n",
    "- images_list : chemins des images avec au moins NUM_PIXELS de feu\n",
    "- masks_list  : chemins des masques correspondants\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# === Adapte ici ===\n",
    "MASK_PATH = r\"C:/content/masks\"\n",
    "IMAGE_PATH = r\"C:/content/images\"\n",
    "MASK_ALGORITHM = \"Kumar-Roy\"  # adapte si besoin\n",
    "NUM_PIXELS = 20\n",
    "\n",
    "\n",
    "def get_mask_arr(path):\n",
    "    \"\"\"Charge un masque en array numpy\"\"\"\n",
    "    with rasterio.open(path) as src:\n",
    "        img = src.read().transpose((1, 2, 0))\n",
    "        seg = np.array(img, dtype=int)\n",
    "        return seg[:, :, 0]\n",
    "\n",
    "\n",
    "def find_fire_images_and_masks(mask_path=MASK_PATH, image_path=IMAGE_PATH,\n",
    "                               num_pixels=NUM_PIXELS, algo=MASK_ALGORITHM):\n",
    "    \"\"\"\n",
    "    Retourne deux listes :\n",
    "    - images_list\n",
    "    - masks_list\n",
    "    contenant uniquement les chemins correspondant aux masques\n",
    "    avec au moins num_pixels pixels feu.\n",
    "    \"\"\"\n",
    "    masks_files = glob(os.path.join(mask_path, f\"*{algo}*.tif\"))\n",
    "    images_list, masks_list = [], []\n",
    "\n",
    "    for mask_file in masks_files:\n",
    "        mask = get_mask_arr(mask_file)\n",
    "        count = (mask > 0).sum()\n",
    "\n",
    "        if count > num_pixels:\n",
    "            # Construire chemin image correspondant\n",
    "            mask_name = os.path.basename(mask_file)\n",
    "            image_name = mask_name.replace(f\"_{algo}_\", \"_\")\n",
    "            image_file = os.path.join(image_path, image_name)\n",
    "\n",
    "            if os.path.exists(image_file):  # sécurité\n",
    "                images_list.append(image_file)\n",
    "                masks_list.append(mask_file)\n",
    "\n",
    "    return images_list, masks_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OALFSyL1Q9B2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Normalisation max Landsat pixel value (ex: 65535 pour 16-bit)\n",
    "MAX_PIXEL_VALUE = 65535.0\n",
    "\n",
    "def get_img_762bands(path):\n",
    "    img = rasterio.open(path).read((7,6,2)).transpose((1, 2, 0))\n",
    "    img = np.float32(img)/MAX_PIXEL_VALUE\n",
    "\n",
    "    return img\n",
    "\n",
    "def get_mask_arr(path):\n",
    "    img = rasterio.open(path).read().transpose((1, 2, 0))\n",
    "    seg = np.float32(img)\n",
    "    return seg\n",
    "\n",
    "def load_all_images_and_masks(images_path_list, masks_path_list, size=(256, 256)):\n",
    "    \"\"\"Retourne X (images) et y (masques) en numpy arrays, avec barre de progression.\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    fopen_image = get_img_762bands\n",
    "    fopen_mask = get_mask_arr\n",
    "\n",
    "    for img_path, mask_path in tqdm(zip(images_path_list, masks_path_list),\n",
    "                                    total=len(images_path_list),\n",
    "                                    desc=\"Chargement des images et masques\"):\n",
    "        try:\n",
    "            img = fopen_image(img_path)\n",
    "            mask = fopen_mask(mask_path)\n",
    "            X.append(img)\n",
    "            y.append(mask)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Impossible de charger {img_path} ou {mask_path} | Erreur: {e}\")\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQDjEJFaeMla"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Double Convolution Block\n",
    "# -----------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Attention Gate\n",
    "# -----------------------------\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(gating_channels, inter_channels, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(inter_channels)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, inter_channels, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(inter_channels)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(inter_channels, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Attention U-Net\n",
    "# -----------------------------\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(img_ch, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = ConvBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = ConvBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv4 = ConvBlock(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        # Attention + Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.att4 = AttentionGate(512, 512, 256)\n",
    "        self.up_conv4 = ConvBlock(1024, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.att3 = AttentionGate(256, 256, 128)\n",
    "        self.up_conv3 = ConvBlock(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.att2 = AttentionGate(128, 128, 64)\n",
    "        self.up_conv2 = ConvBlock(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.att1 = AttentionGate(64, 64, 32)\n",
    "        self.up_conv1 = ConvBlock(128, 64)\n",
    "\n",
    "        # Final conv\n",
    "        self.out_conv = nn.Conv2d(64, output_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.conv1(x)\n",
    "        p1 = self.pool1(x1)\n",
    "\n",
    "        x2 = self.conv2(p1)\n",
    "        p2 = self.pool2(x2)\n",
    "\n",
    "        x3 = self.conv3(p2)\n",
    "        p3 = self.pool3(x3)\n",
    "\n",
    "        x4 = self.conv4(p3)\n",
    "        p4 = self.pool4(x4)\n",
    "\n",
    "        x5 = self.conv5(p4)\n",
    "\n",
    "        # Decoder with Attention\n",
    "        d4 = self.up4(x5)\n",
    "        x4 = self.att4(x4, d4)\n",
    "        d4 = torch.cat((x4, d4), dim=1)\n",
    "        d4 = self.up_conv4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        x3 = self.att3(x3, d3)\n",
    "        d3 = torch.cat((x3, d3), dim=1)\n",
    "        d3 = self.up_conv3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        x2 = self.att2(x2, d2)\n",
    "        d2 = torch.cat((x2, d2), dim=1)\n",
    "        d2 = self.up_conv2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        x1 = self.att1(x1, d1)\n",
    "        d1 = torch.cat((x1, d1), dim=1)\n",
    "        d1 = self.up_conv1(d1)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        out = F.interpolate(out, size=(256, 256), mode=\"bilinear\", align_corners=False)\n",
    "        return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Test\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = AttentionUNet(img_ch=3, output_ch=1)\n",
    "    inp = torch.randn(1, 3, 256, 256)\n",
    "    out = model(inp)\n",
    "    print(\"Input:\", inp.shape)\n",
    "    print(\"Output:\", out.shape)  # (1, 1, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shuRLM3PWKEw"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Nettoyer le GPU \n",
    "# -----------------------------\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Créer le modèle\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AttentionUNet(img_ch=3, output_ch=1).to(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "86kFP5TceSYM",
    "outputId": "47acffe6-69eb-4bf3-eb32-ca6ca0d8e33f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dice Loss (pour segmentation binaire)\n",
    "# -----------------------------\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        y_true_f = y_true.view(-1)\n",
    "        y_pred_f = y_pred.view(-1)\n",
    "        intersection = (y_true_f * y_pred_f).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (y_true_f.sum() + y_pred_f.sum() + self.smooth)\n",
    "        return 1 - dice   # On retourne la loss, donc 1 - Dice\n",
    "        \n",
    "\n",
    "# -----------------------------\n",
    "# Focal Loss\n",
    "# -----------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2., alpha=0.25, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        y_pred = y_pred.clamp(1e-7, 1 - 1e-7)\n",
    "\n",
    "        bce = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        pt = torch.where(y_true == 1, y_pred, 1 - y_pred)\n",
    "        focal_term = self.alpha * (1 - pt) ** self.gamma\n",
    "        loss = focal_term * bce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Combined Loss : Focal + Dice\n",
    "# -----------------------------\n",
    "class FocalDiceLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2., lambda_dice=1.0, lambda_focal=1.0):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(gamma=gamma, alpha=alpha)\n",
    "        self.dice = DiceLoss()\n",
    "        self.lambda_dice = lambda_dice\n",
    "        self.lambda_focal = lambda_focal\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss_focal = self.focal(y_pred, y_true)\n",
    "        loss_dice = self.dice(y_pred, y_true)\n",
    "        return self.lambda_focal * loss_focal + self.lambda_dice * loss_dice\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Exemple d’utilisation\n",
    "# -----------------------------\n",
    "criterion = FocalDiceLoss(alpha=0.75, gamma=2., lambda_dice=1.0, lambda_focal=1.0)\n",
    "\n",
    "y_true = torch.randint(0, 2, (8, 1, 256, 256), dtype=torch.float32)\n",
    "y_pred = torch.rand(8, 1, 256, 256)\n",
    "\n",
    "loss = criterion(y_pred, y_true)\n",
    "print(\"Combined Focal + Dice Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU5HMRHobxei"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cleanup_dataset(\n",
    "    image_path='images/patches',\n",
    "    mask_path='masks/patches',\n",
    "    compressed_path='compressed'\n",
    "):\n",
    "    \"\"\"\n",
    "    Supprime tous les fichiers contenus dans :\n",
    "    - images/patches\n",
    "    - masks/patches\n",
    "    - compressed\n",
    "    \n",
    "    Et recrée les dossiers vides pour éviter les erreurs plus tard.\n",
    "    \"\"\"\n",
    "    removed = 0\n",
    "    for folder in [image_path, mask_path, compressed_path]:\n",
    "        if os.path.exists(folder):\n",
    "            for f in os.listdir(folder):\n",
    "                file_path = os.path.join(folder, f)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                        removed += 1\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                        removed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Impossible de supprimer {file_path} : {e}\")\n",
    "        else:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Cleanup terminé. {removed} fichiers supprimés.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQxtChNoXzeU",
    "outputId": "e9a40ea6-280b-45f4-d282-09f4257890c9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset personnalisé\n",
    "# -----------------------------\n",
    "class FireDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "        return img.float(), mask.float()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dice coefficient (métrique)\n",
    "# -----------------------------\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).float()  # binarisation\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Boucle principale\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Ici on utilise ta loss définie ailleurs\n",
    "criterion = FocalDiceLoss(alpha=0.75, gamma=2., lambda_dice=1.0, lambda_focal=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Régions à traiter\n",
    "regions = [\n",
    "    \"South_America\", \n",
    "\n",
    "    #\"Asia2\", \"Asia3\", \"Asia4\", \n",
    "    #\"North_America1\", \"North_America2\"\n",
    "]\n",
    "cleanup_dataset(\n",
    "        image_path=\"c:/content/images/patches\",\n",
    "        mask_path=\"c:/content/masks/patches\",\n",
    "        compressed_path=\"c:/content/compressed\"\n",
    "    )\n",
    "\n",
    "for region in regions:\n",
    "    print(f\"\\n === Traitement de {region} ===\")\n",
    "\n",
    "    # 1. Télécharger dataset\n",
    "    download_regions([region], output_dir=\"c:/content/compressed\")\n",
    "    unzip_dataset(full_dataset=True,\n",
    "                 compressed_dir=\"c:/content/compressed\",\n",
    "                \n",
    "                 output_dir=\"c:/content\")\n",
    "\n",
    "    # 2. Filtrer images/masks\n",
    "    image_paths, masks_paths = find_fire_images_and_masks(\n",
    "        image_path=\"c:/content/images/patches\",\n",
    "        mask_path=\"c:/content/masks/patches\"\n",
    "    )\n",
    "    X, y = load_all_images_and_masks(image_paths, masks_paths, size=(256, 256))\n",
    "\n",
    "    # Créer dataset\n",
    "    dataset = FireDataset(X, y, transform=transform)\n",
    "\n",
    "    # Split train/test\n",
    "    test_size = int(0.05 * len(dataset))\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 3. Entraîner modèle\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        running_dice = 0.0\n",
    "        for imgs, masks in train_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "           \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "\n",
    "            # --- Calcul du Dice ---\n",
    "            dice = dice_coef(masks, outputs)\n",
    "            running_dice += dice.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        avg_dice = running_dice / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Dice: {avg_dice:.4f}\")\n",
    "\n",
    "    # 4. Supprimer dataset pour libérer RAM/disque\n",
    "    del dataset, train_loader, X, y\n",
    "    torch.cuda.empty_cache()\n",
    "    cleanup_dataset(\n",
    "        image_path=\"c:/content/images/patches\",\n",
    "        mask_path=\"c:/content/masks/patches\",\n",
    "        compressed_path=\"c:/content/compressed\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zgd9NUZeY4X"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# === 5. Tester le modèle sur un batch ===\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    images, masks = next(iter(test_loader))\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    preds = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "# === 6. Affichage ===\n",
    "idx = np.random.randint(0, images.shape[0])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(images[idx].cpu().permute(1, 2, 0)[..., :3])  # 3 canaux\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Mask réel\")\n",
    "plt.imshow(masks[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Mask prédit\")\n",
    "plt.imshow(preds[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Télécharger le subset sample ===\n",
    "samples_zip = download_regions(region_list=None, subset=True, output_dir=COMPRESSED_DIR)\n",
    "\n",
    "# === 2. Dézipper ===\n",
    "unzip_dataset(full_dataset=False, samples_zip_path=samples_zip)\n",
    "\n",
    " #=== 3. Charger images + masques filtrés ===\n",
    "image_paths, mask_paths = find_fire_images_and_masks(\n",
    "    image_path=os.path.join(IMAGES_DIR, \"patches\"),\n",
    "    mask_path=os.path.join(MASKS_DIR, \"patches\"),\n",
    "    num_pixels=10)\n",
    "\n",
    "print(f\"Images trouvées: {len(image_paths)}\")\n",
    "print(f\"Masks trouvés: {len(mask_paths)}\")\n",
    "X, y = load_all_images_and_masks(image_paths, mask_paths, size=(256, 256))\n",
    "dataset = FireDataset(X, y, transform=transform)\n",
    "test_size = int(0.95 * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0rYGSwCu0uF"
   },
   "outputs": [],
   "source": [
    "# Évaluation du modèle sur le test_loader\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_dice = 0.0\n",
    "    total_loss = 0.0\n",
    "    criterion = FocalDiceLoss(alpha=0.75, gamma=2., lambda_dice=1.0, lambda_focal=1.0) # même loss qu'à l'entraînement\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Activation sigmoid\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Dice score\n",
    "            dice = dice_coef(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_dice = total_dice / len(test_loader)\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Dice Coefficient: {avg_dice:.4f}\")\n",
    "    return avg_loss, avg_dice\n",
    "\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"attention_unet_weights.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Modèle sauvegardé dans {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch-nightly]",
   "language": "python",
   "name": "conda-env-torch-nightly-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
