{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iy5cSbQCNz7K",
    "outputId": "de218269-5a63-466a-d6a0-4dae1bf66837"
   },
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!pip install opencv-python tensorflow numpy pandas scikit-learn matplotlib rasterio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qDNyavpMICS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gdown\n",
    "from glob import glob\n",
    "import zipfile\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import rasterio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e75e8f61"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "BASE_URL = 'https://drive.google.com/uc?id={}'\n",
    "BASE_PATH = os.path.abspath(\"C:/content\")\n",
    "COMPRESSED_DIR = os.path.join(BASE_PATH, \"compressed\")\n",
    "\n",
    "REGIONS = {\n",
    "    'South_America': '1hnBUmYkvFYohTf9hTnKiyp2MXVevglK7',\n",
    "    'Oceania': '1xHYTICHKU0u3-kIrq-pM9k0YaeQt60Bt',\n",
    "    'North_America1': '1BXRGldTdGGNeWDOFqNnmiNPuPQjweB2M',\n",
    "    'North_America2': '1zW_pEIggJ5Li7uQX9XKMfHkcgL3kiUoi',\n",
    "    'Africa': '1Ng3JwsjJPApshk8lJdGcsNHI52NaEMDX',\n",
    "    'Europe': '1vANGtfuEdn0ZnILA6BYXW1_7jt8CU0gA',\n",
    "    'Asia1': '1xgOQkeQIswq3hLBhNzuNPTtsL4ZavuC3',\n",
    "    'Asia2': '1w_wv0_QZhnH9jO1ygJg6ssTrXupIbTHp',\n",
    "    'Asia3': '1heefSuPsnLZkNSJ2jTa4M-jGWo_9nAri',\n",
    "    'Asia4': '1lyR6y6u8tSozfv3AQJ1PuUcdR0BU9yUk',\n",
    "    'Asia5': '1Y1UysFrZ8AiugKvDpWI3nHcjo7-CQ4Dp',\n",
    "}\n",
    "\n",
    "SUBSET_SAMPLES = {\n",
    "    'samples': '1gwQdhXrxCybcO16vem09DfW5fPadAA_p',\n",
    "}\n",
    "def download_file(file_id, output):\n",
    "    \"\"\"\n",
    "    Download a file from Google Drive by its ID.\n",
    "    Args:\n",
    "        file_id (str): Google Drive file ID.\n",
    "        output (str): Output path for the downloaded file.\n",
    "    \"\"\"\n",
    "    url = BASE_URL.format(file_id)\n",
    "    print(f\"Downloading from Google Drive: {output}\")\n",
    "    gdown.download(url, output, quiet=False, fuzzy=True)\n",
    "\n",
    "\n",
    "def download_regions(region_list, output_dir=COMPRESSED_DIR, subset=False):\n",
    "    \"\"\"\n",
    "    Download specified datasets or subset samples.\n",
    "    Args:\n",
    "        region_list (list): List of regions to download.\n",
    "        output_dir (str): Output directory.\n",
    "        subset (bool): If True, download only the subset samples.zip.\n",
    "    Returns:\n",
    "        List of downloaded file paths or path to subset zip.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if subset:\n",
    "        zip_file_name = 'samples.zip'\n",
    "        output = os.path.join(BASE_PATH, zip_file_name)\n",
    "        print(\"Downloading subset samples...\")\n",
    "        download_file(SUBSET_SAMPLES['samples'], output)\n",
    "        return output\n",
    "    else:\n",
    "        downloaded_files = []\n",
    "        for region in region_list:\n",
    "            if region not in REGIONS:\n",
    "                print(f\"Unknown region {region}. Skipped.\")\n",
    "                continue\n",
    "            zip_file_name = f'{region}.zip'\n",
    "            output = os.path.join(output_dir, zip_file_name)\n",
    "            print(f\"=== Downloading region: {region} ===\")\n",
    "            download_file(REGIONS[region], output)\n",
    "            downloaded_files.append(output)\n",
    "        return downloaded_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x7gSG4VPRg-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BASE_PATH = os.path.abspath(\"C:/content\")\n",
    "COMPRESSED_DIR = os.path.join(BASE_PATH, \"compressed\")\n",
    "IMAGES_DIR = os.path.join(BASE_PATH, \"images\")\n",
    "MASKS_DIR = os.path.join(BASE_PATH, \"masks\")\n",
    "\n",
    "# Function to unzip either the full dataset or the subset samples\n",
    "# Args:\n",
    "#   full_dataset (bool): If True, unzip full dataset, else subset\n",
    "#   samples_zip_path (str): Path to samples.zip\n",
    "#   compressed_dir (str): Path to downloaded continent zips\n",
    "#   output_dir (str): Final output directory\n",
    "# Usage: unzip_dataset(full_dataset=True) or unzip_dataset(full_dataset=False)\n",
    "def unzip_dataset(full_dataset=True,\n",
    "                  samples_zip_path=os.path.join(BASE_PATH, \"samples.zip\"),\n",
    "                  compressed_dir=COMPRESSED_DIR,\n",
    "                  output_dir=BASE_PATH):\n",
    "    if full_dataset:\n",
    "        print('Unzipping Full Dataset...')\n",
    "        # Create fixed directories for images and masks\n",
    "        patches_output_dir = os.path.join(IMAGES_DIR, 'patches')\n",
    "        masks_output_dir = os.path.join(MASKS_DIR, 'patches')\n",
    "        voting_output_dir = os.path.join(MASKS_DIR, 'voting')\n",
    "        intersection_output_dir = os.path.join(MASKS_DIR, 'intersection')\n",
    "        for d in [patches_output_dir, masks_output_dir, voting_output_dir, intersection_output_dir]:\n",
    "            os.makedirs(d, exist_ok=True)\n",
    "        zips_continents = glob(os.path.join(compressed_dir, '*.zip'))\n",
    "        tmp_dir = os.path.join(output_dir, 'tmp')\n",
    "        tmp_derivates = os.path.join(output_dir, 'tmp_derivates')\n",
    "        print(f'Unzip images to {patches_output_dir}')\n",
    "        print(f'Unzip masks to {masks_output_dir}')\n",
    "        total_files = 0\n",
    "        for zip_continent in zips_continents:\n",
    "            print(f'Unzipping: {zip_continent}')\n",
    "            os.makedirs(tmp_dir, exist_ok=True)\n",
    "            with zipfile.ZipFile(zip_continent, 'r') as zip_ref:\n",
    "                print(f'Num zipped files: {len(zip_ref.namelist())}')\n",
    "                zip_ref.extractall(tmp_dir)\n",
    "            patches_zips = glob(os.path.join(tmp_dir, '*.zip'))\n",
    "            print(f'Num. of zips unpacked: {len(patches_zips)}')\n",
    "            print('Unzipping patches...')\n",
    "            num_files = 0\n",
    "            for patches_zip in patches_zips:\n",
    "                output_dir_zip = patches_output_dir\n",
    "                if patches_zip.endswith('masks_derivates.zip'):\n",
    "                    with zipfile.ZipFile(patches_zip, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(tmp_derivates)\n",
    "                        num_files += len(zip_ref.namelist())\n",
    "                    derivate_patches = glob(os.path.join(tmp_derivates, '*.tif'))\n",
    "                    for derivate_patch in derivate_patches:\n",
    "                        if '_voting_' in derivate_patch.lower():\n",
    "                            shutil.move(derivate_patch, derivate_patch.replace(tmp_derivates, voting_output_dir))\n",
    "                        elif '_intersection_' in derivate_patch.lower():\n",
    "                            shutil.move(derivate_patch, derivate_patch.replace(tmp_derivates, intersection_output_dir))\n",
    "                    shutil.rmtree(tmp_derivates)\n",
    "                    continue\n",
    "                if patches_zip.endswith('masks.zip'):\n",
    "                    output_dir_zip = masks_output_dir\n",
    "                with zipfile.ZipFile(patches_zip, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(output_dir_zip)\n",
    "                    num_files += len(zip_ref.namelist())\n",
    "            total_files += num_files\n",
    "            print(f'Zip: {zip_continent} - Patches: {num_files}')\n",
    "            shutil.rmtree(tmp_dir)\n",
    "        print(f'Total files unzipped: {total_files}')\n",
    "        print('Full Dataset Ready!')\n",
    "    else:\n",
    "        print(\"Unzipping subset samples...\")\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            with zipfile.ZipFile(samples_zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(tmpdirname)\n",
    "            images_path = IMAGES_DIR\n",
    "            masks_path = MASKS_DIR\n",
    "            manual_annotations_path = os.path.join(BASE_PATH, 'manual_annotations', 'patches')\n",
    "            for d in [images_path, masks_path, manual_annotations_path]:\n",
    "                os.makedirs(d, exist_ok=True)\n",
    "            image_zip = os.path.join(tmpdirname, 'samples', 'images', 'patches.zip')\n",
    "            with zipfile.ZipFile(image_zip, 'r') as zip_ref:\n",
    "                zip_ref.extractall(images_path)\n",
    "            masks_zips = glob(os.path.join(tmpdirname, 'samples', 'masks', '*.zip'))\n",
    "            for mask_zip in masks_zips:\n",
    "                with zipfile.ZipFile(mask_zip, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(masks_path)\n",
    "            masks = glob(os.path.join(masks_path, 'patches', '*_GOLI_v2_*.tif'))\n",
    "            for mask in masks:\n",
    "                mask_name = os.path.basename(mask)\n",
    "                os.rename(\n",
    "                    os.path.join(masks_path, 'patches', mask_name),\n",
    "                    os.path.join(masks_path, 'patches', mask_name.replace('GOLI_v2', 'Kumar-Roy'))\n",
    "                )\n",
    "            manual_annotations_zips = glob(os.path.join(tmpdirname, 'samples', 'manual_annotations', '*.zip'))\n",
    "            for manual_ann_zip in manual_annotations_zips:\n",
    "                with zipfile.ZipFile(manual_ann_zip, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(manual_annotations_path)\n",
    "        print(\"Subset unzipped successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KllxJdToI0f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns two lists:\n",
    "- images_list: paths to images with at least NUM_PIXELS fire pixels\n",
    "- masks_list: paths to corresponding masks\n",
    "\"\"\"\n",
    "\n",
    "# Set paths and parameters\n",
    "\n",
    "\n",
    "\n",
    "MASK_PATH = r\"C:/content/masks/patches\"\n",
    "IMAGE_PATH = r\"C:/content/images/patches\"\n",
    "MASK_ALGORITHM = \"Kumar-Roy\"  # change if needed\n",
    "NUM_PIXELS = 20\n",
    "\n",
    "# Load a mask as a numpy array\n",
    "def get_mask_arr(path):\n",
    "    \"\"\"\n",
    "    Load a mask as a numpy array.\n",
    "    Args:\n",
    "        path (str): Path to mask file.\n",
    "    Returns:\n",
    "        np.ndarray: Mask array.\n",
    "    \"\"\"\n",
    "    with rasterio.open(path) as src:\n",
    "        img = src.read().transpose((1, 2, 0))\n",
    "        seg = np.array(img, dtype=int)\n",
    "        return seg[:, :, 0]\n",
    "\n",
    "# Find images and masks with at least num_pixels fire pixels\n",
    "def find_fire_images_and_masks(mask_path=MASK_PATH, \n",
    "                               image_path=IMAGE_PATH,\n",
    "                               num_pixels=NUM_PIXELS, \n",
    "                               algo=MASK_ALGORITHM,\n",
    "                               manual=False):\n",
    "    \"\"\"\n",
    "    Returns two lists:\n",
    "    - images_list: paths to image files\n",
    "    - masks_list: paths to mask files\n",
    "    \n",
    "    Args:\n",
    "        mask_path (str): Path to masks directory.\n",
    "        image_path (str): Path to images directory.\n",
    "        num_pixels (int): Minimum number of fire pixels required.\n",
    "        algo (str): Algorithm name used in mask filenames (ignored if manual=True).\n",
    "        manual (bool): If True, search in manual_annotations/patches.\n",
    "    \"\"\"\n",
    "    if manual:\n",
    "        BASE_PATH = r\"C:\\content\"   \n",
    "        mask_path = os.path.join(BASE_PATH, \"manual_annotations\", \"patches\", \"manual_annotations_patches\")\n",
    "        mask_path = os.path.normpath(mask_path)  \n",
    "        masks_files = glob(os.path.join(mask_path, \"*.tif\"))\n",
    "        image_path = os.path.normpath(\n",
    "        os.path.join(\"c:/content\", \"manual_annotations\", \"patches\", \"landsat_patches\"))\n",
    "\n",
    "    \n",
    "    else:\n",
    "        # Algorithm-generated masks follow the naming convention with algo\n",
    "        masks_files = glob(os.path.join(mask_path, f\"*{algo}*.tif\"))\n",
    "    \n",
    "    images_list, masks_list = [], []\n",
    "    for mask_file in masks_files:\n",
    "        mask = get_mask_arr(mask_file)\n",
    "        count = (mask > 0).sum()\n",
    "        \n",
    "        # Keep only masks with enough fire pixels\n",
    "        if count > num_pixels:\n",
    "            mask_name = os.path.basename(mask_file)\n",
    "            \n",
    "            if manual:\n",
    "                # Remove version tags like \"_v1_\", \"_v2_\", etc. from mask filename\n",
    "                image_name = re.sub(r\"_v\\d+_\", \"_\", mask_name)\n",
    "            else:\n",
    "                # Algorithmic masks need the algo tag removed to match image names\n",
    "                image_name = mask_name.replace(f\"_{algo}_\", \"_\")\n",
    "            \n",
    "            image_file = os.path.join(image_path, image_name)\n",
    "            if os.path.exists(image_file):\n",
    "                images_list.append(image_file)\n",
    "                masks_list.append(mask_file)\n",
    "  \n",
    "    return images_list, masks_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OALFSyL1Q9B2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Normalisation max Landsat pixel value (ex: 65535 pour 16-bit)\n",
    "MAX_PIXEL_VALUE = 65535.0\n",
    "\n",
    "def get_img_762bands(path):\n",
    "    \"\"\"\n",
    "    Load image with bands 7, 6, 2 and normalize.\n",
    "    Args:\n",
    "        path (str): Path to image file.\n",
    "    Returns:\n",
    "        np.ndarray: Normalized image array.\n",
    "    \"\"\"\n",
    "    img = rasterio.open(path).read((7,6,2)).transpose((1, 2, 0))\n",
    "    img = np.float32(img)/MAX_PIXEL_VALUE\n",
    "    return img\n",
    "\n",
    "def get_mask_arr(path):\n",
    "    \"\"\"\n",
    "    Load mask as float32 numpy array.\n",
    "    Args:\n",
    "        path (str): Path to mask file.\n",
    "    Returns:\n",
    "        np.ndarray: Mask array.\n",
    "    \"\"\"\n",
    "    img = rasterio.open(path).read().transpose((1, 2, 0))\n",
    "    seg = np.float32(img)\n",
    "    return seg\n",
    "\n",
    "def load_all_images_and_masks(images_path_list, masks_path_list, size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Load all images and masks into numpy arrays with progress bar.\n",
    "    Args:\n",
    "        images_path_list (list): List of image paths.\n",
    "        masks_path_list (list): List of mask paths.\n",
    "        size (tuple): Target size.\n",
    "    Returns:\n",
    "        X, y: Arrays of images and masks.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    fopen_image = get_img_762bands\n",
    "    fopen_mask = get_mask_arr\n",
    "    for img_path, mask_path in tqdm(zip(images_path_list, masks_path_list),\n",
    "                                    total=len(images_path_list),\n",
    "                                    desc=\"Loading images and masks\"):\n",
    "        try:\n",
    "            img = fopen_image(img_path)\n",
    "            mask = fopen_mask(mask_path)\n",
    "            X.append(img)\n",
    "            y.append(mask)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Could not load {img_path} or {mask_path} | Error: {e}\")\n",
    "            \n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQDjEJFaeMla"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Double Convolution Block for U-Net encoder/decoder\n",
    "# -----------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Attention Gate\n",
    "# -----------------------------\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, in_channels, gating_channels, inter_channels):\n",
    "        super().__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(gating_channels, inter_channels, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(inter_channels)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, inter_channels, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(inter_channels)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(inter_channels, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x * psi\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Attention U-Net\n",
    "# -----------------------------\n",
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, img_ch=3, output_ch=1):\n",
    "        \"\"\"\n",
    "        Attention U-Net model for segmentation\n",
    "\n",
    "        Args:\n",
    "            img_ch (int): Number of input image channels (default: 3)\n",
    "            output_ch (int): Number of output channels (default: 1)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(img_ch, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = ConvBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = ConvBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv4 = ConvBlock(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv5 = ConvBlock(512, 1024)\n",
    "\n",
    "        # Attention + Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.att4 = AttentionGate(512, 512, 256)\n",
    "        self.up_conv4 = ConvBlock(1024, 512)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.att3 = AttentionGate(256, 256, 128)\n",
    "        self.up_conv3 = ConvBlock(512, 256)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.att2 = AttentionGate(128, 128, 64)\n",
    "        self.up_conv2 = ConvBlock(256, 128)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.att1 = AttentionGate(64, 64, 32)\n",
    "        self.up_conv1 = ConvBlock(128, 64)\n",
    "\n",
    "        # Final conv\n",
    "        self.out_conv = nn.Conv2d(64, output_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        p1 = self.pool1(x1)\n",
    "\n",
    "        x2 = self.conv2(p1)\n",
    "        p2 = self.pool2(x2)\n",
    "\n",
    "        x3 = self.conv3(p2)\n",
    "        p3 = self.pool3(x3)\n",
    "\n",
    "        x4 = self.conv4(p3)\n",
    "        p4 = self.pool4(x4)\n",
    "\n",
    "        x5 = self.conv5(p4)\n",
    "\n",
    "        d4 = self.up4(x5)\n",
    "        x4 = self.att4(x4, d4)\n",
    "        d4 = torch.cat((x4, d4), dim=1)\n",
    "        d4 = self.up_conv4(d4)\n",
    "\n",
    "        d3 = self.up3(d4)\n",
    "        x3 = self.att3(x3, d3)\n",
    "        d3 = torch.cat((x3, d3), dim=1)\n",
    "        d3 = self.up_conv3(d3)\n",
    "\n",
    "        d2 = self.up2(d3)\n",
    "        x2 = self.att2(x2, d2)\n",
    "        d2 = torch.cat((x2, d2), dim=1)\n",
    "        d2 = self.up_conv2(d2)\n",
    "\n",
    "        d1 = self.up1(d2)\n",
    "        x1 = self.att1(x1, d1)\n",
    "        d1 = torch.cat((x1, d1), dim=1)\n",
    "        d1 = self.up_conv1(d1)\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        out = F.interpolate(out, size=(256, 256), mode=\"bilinear\", align_corners=False)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shuRLM3PWKEw"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Clear GPU cache\n",
    "# -----------------------------\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Create the model and set device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AttentionUNet(img_ch=3, output_ch=1).to(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "86kFP5TceSYM",
    "outputId": "47acffe6-69eb-4bf3-eb32-ca6ca0d8e33f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dice Loss for binary segmentation\n",
    "# -----------------------------\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        y_true_f = y_true.view(-1)\n",
    "        y_pred_f = y_pred.view(-1)\n",
    "        intersection = (y_true_f * y_pred_f).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (y_true_f.sum() + y_pred_f.sum() + self.smooth)\n",
    "        return 1 - dice   # Return the loss, so 1 - Dice\n",
    "        \n",
    "\n",
    "# -----------------------------\n",
    "# Focal Loss for binary segmentation\n",
    "# -----------------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2., alpha=0.25, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "        y_pred = y_pred.clamp(1e-7, 1 - 1e-7)\n",
    "\n",
    "        bce = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "        pt = torch.where(y_true == 1, y_pred, 1 - y_pred)\n",
    "        focal_term = self.alpha * (1 - pt) ** self.gamma\n",
    "        loss = focal_term * bce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Combined Focal and Dice Loss for segmentation\n",
    "# -----------------------------\n",
    "class FocalDiceLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2., lambda_dice=1.0, lambda_focal=1.0):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(gamma=gamma, alpha=alpha)\n",
    "        self.dice = DiceLoss()\n",
    "        self.lambda_dice = lambda_dice\n",
    "        self.lambda_focal = lambda_focal\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss_focal = self.focal(y_pred, y_true)\n",
    "        loss_dice = self.dice(y_pred, y_true)\n",
    "        return self.lambda_focal * loss_focal + self.lambda_dice * loss_dice\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uU5HMRHobxei"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cleanup_dataset(\n",
    "    image_path='images/patches',\n",
    "    mask_path='masks/patches',\n",
    "    compressed_path='compressed'\n",
    "):\n",
    "    \"\"\"\n",
    "    Delete all files in:\n",
    "    - images/patches\n",
    "    - masks/patches\n",
    "    - compressed\n",
    "    \n",
    "    And recreate empty folders to avoid errors later.\n",
    "    \"\"\"\n",
    "    removed = 0\n",
    "    for folder in [image_path, mask_path, compressed_path]:\n",
    "        if os.path.exists(folder):\n",
    "            for f in os.listdir(folder):\n",
    "                file_path = os.path.join(folder, f)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                        removed += 1\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                        removed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not delete {file_path}: {e}\")\n",
    "        else:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Cleanup complete. {removed} files deleted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQxtChNoXzeU",
    "outputId": "e9a40ea6-280b-45f4-d282-09f4257890c9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Custom Dataset for fire images and masks\n",
    "class FireDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            mask = self.transform(mask)\n",
    "        return img.float(), mask.float()\n",
    "\n",
    "# Dice coefficient metric for segmentation\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).float()  # binarization\n",
    "    y_true_f = y_true.view(-1)\n",
    "    y_pred_f = y_pred.view(-1)\n",
    "    intersection = (y_true_f * y_pred_f).sum()\n",
    "    return (2. * intersection + smooth) / (y_true_f.sum() + y_pred_f.sum() + smooth)\n",
    "\n",
    "# Main training loop\n",
    "# Trains the model for each region, splits data, and cleans up after each region\n",
    "# Uses DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "criterion = DiceLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "regions = [\n",
    "    \"South_America\",\n",
    "    # \"Asia2\", \"Asia3\", \"Asia4\",\n",
    "    # \"North_America1\", \"North_America2\"\n",
    "]\n",
    "cleanup_dataset(\n",
    "    image_path=\"c:/content/images/patches\",\n",
    "    mask_path=\"c:/content/masks/patches\",\n",
    "    compressed_path=\"c:/content/compressed\"\n",
    ") \n",
    "for region in regions:\n",
    "    print(f\"\\n === Processing {region} ===\")\n",
    "    download_regions([region], output_dir=\"c:/content/compressed\")\n",
    "   \n",
    "   \n",
    "    unzip_dataset(full_dataset=True,\n",
    "                compressed_dir=\"c:/content/compressed\",\n",
    "              output_dir=\"c:/content\") \n",
    "    \n",
    "    image_paths, masks_paths = find_fire_images_and_masks(\n",
    "\n",
    "    manual=False  ,\n",
    ")\n",
    "    print(f\"Images found: {len(image_paths)}\")\n",
    "    print(f\"Masks found: {len(masks_paths)}\")\n",
    "\n",
    "    X, y = load_all_images_and_masks(image_paths, masks_paths, size=(256, 256))\n",
    "    dataset = FireDataset(X, y, transform=transform)\n",
    "    test_size = int(0.05 * len(dataset))\n",
    "    train_size = len(dataset) - test_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        running_dice = 0.0\n",
    "        for imgs, masks in train_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            dice = dice_coef(masks, outputs)\n",
    "            running_dice += dice.item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        avg_dice = running_dice / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}, Dice: {avg_dice:.4f}\")\n",
    "    del dataset, train_loader, X, y\n",
    "    torch.cuda.empty_cache()\n",
    "    cleanup_dataset(\n",
    "        image_path=\"c:/content/images/patches\",\n",
    "        mask_path=\"c:/content/masks/patches\",\n",
    "        compressed_path=\"c:/content/compressed\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zgd9NUZeY4X"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Test the model on a batch\n",
    "# -----------------------------\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, masks = next(iter(test_loader))\n",
    "    images, masks = images.to(device), masks.to(device)\n",
    "    outputs = model(images)\n",
    "    preds = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "# -----------------------------\n",
    "# Visualization\n",
    "# -----------------------------\n",
    "idx = np.random.randint(0, images.shape[0])\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Image\")\n",
    "plt.imshow(images[idx].cpu().permute(1, 2, 0)[..., :3])  # 3 channels\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"True Mask\")\n",
    "plt.imshow(masks[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.imshow(preds[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Download the subset sample ===\n",
    "samples_zip = download_regions(region_list=None, subset=True, output_dir=COMPRESSED_DIR)\n",
    "\n",
    "# === 2. Unzip ===\n",
    "unzip_dataset(full_dataset=False, samples_zip_path=samples_zip)\n",
    "\n",
    "# === 3. Load filtered images + masks ===\n",
    "image_paths, mask_paths = find_fire_images_and_masks(\n",
    "    image_path=os.path.join(IMAGES_DIR, \"patches\"),\n",
    "    num_pixels=10,\n",
    "    manual=True   \n",
    ")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "print(f\"Images found: {len(image_paths)}\")\n",
    "print(f\"Masks found: {len(mask_paths)}\")\n",
    "X, y = load_all_images_and_masks(image_paths, mask_paths, size=(256, 256))\n",
    "dataset = FireDataset(X, y, transform=transform)\n",
    "test_size = int(1 * len(dataset))\n",
    "train_size = len(dataset) - test_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "#train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0rYGSwCu0uF"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test_loader\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate model on test data.\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        test_loader (DataLoader): Test data loader.\n",
    "        device (torch.device): Device to use.\n",
    "    Returns:\n",
    "        avg_loss, avg_dice: Average loss and dice coefficient.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_dice = 0.0\n",
    "    total_loss = 0.0\n",
    "    criterion = FocalDiceLoss(alpha=0.75, gamma=2., lambda_dice=1.0, lambda_focal=1.0)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in test_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            dice = dice_coef(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_dice = total_dice / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Dice Coefficient: {avg_dice:.4f}\")\n",
    "    return avg_loss, avg_dice\n",
    "\n",
    "evaluate_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"attention_unet_weights.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# initialize the model with the same args used during training\n",
    "model = AttentionUNet().to(device)\n",
    "\n",
    "# load checkpoint (assumes file is in the same directory)\n",
    "checkpoint = torch.load(\"attention_unet_weights.pth\", map_location=device)\n",
    "\n",
    "# if it’s just a state_dict\n",
    "if isinstance(checkpoint, dict) and \"state_dict\" not in checkpoint:\n",
    "    model.load_state_dict(checkpoint)\n",
    "else:  # if it’s a checkpoint with \"state_dict\"\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded and ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-nightly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
